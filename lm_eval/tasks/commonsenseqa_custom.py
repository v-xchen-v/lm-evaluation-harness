"""
COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge
https://arxiv.org/pdf/1811.00937.pdf

The CommonsenseQA is a dataset for commonsense question answering task. The dataset consists of 12,247 questions with 5 choices each. The dataset was generated by Amazon Mechanical Turk workers in the following process (an example is provided in parentheses)
1. a crowd worker observes a source concept from ConceptNet (“River”) and three target concepts (“Waterfall”, “Bridge”, “Valley”) that are all related by the same ConceptNet relation (“AtLocation”),
2. the worker authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not, (“Where on a river can you hold a cup upright to catch water on a sunny day?”, “Where can I stand on a river to see water falling without getting wet?”, “I’m crossing the river, my feet are wet but my body is dry, where am I?”)
3. for each question, another worker chooses one additional distractor from Concept Net (“pebble”, “stream”, “bank”), and the author another distractor (“mountain”, “bottom”, “island”) manually.

Homepage: https://www.tau-nlp.sites.tau.ac.il/commonsenseqa
"""

"""
Talmor, Alon, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. “CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge.” arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1811.00937.
}
"""

from lm_eval.base import LikelihoodOptionContentMultipleChoiceTask, LikelihoodOptionKeyMultipleCircularChoiceTask

_CITATION = """
@misc{talmor2019commonsenseqa,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      eprint={1811.00937},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
"""

class CommonSenseQACustom:
    DATASET_PATH = "commonsense_qa"
    DATASET_NAME = None
    
    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        # test split is "validation"
        return True

    def has_test_docs(self):
        return False

    def training_docs(self):
        if self._training_docs is None:
            self._training_docs = list(map(self._process_doc, self.dataset["train"]))
        return self._training_docs

    def validation_docs(self):
        return map(self._process_doc, self.dataset["validation"])

    def should_decontaminate(self):
        return True

    def doc_to_decontamination_query(self, doc):
        return doc["query"]
    
class CommonSenseQACustomOptionContent(CommonSenseQACustom, LikelihoodOptionContentMultipleChoiceTask):
    VERSION = 0
    
    # prompt = "Question: {question}\nAnswer:"
    def _process_doc(self, doc):
        out_doc = {
            "id": doc["id"],
            "query": f'Question: {doc["question"]}\nAnswer:',
            "choices": doc["choices"]["text"],
            "gold": ["A", "B", "C", "D", "E"].index(doc["answerKey"].strip()),
        }
        return out_doc
    
class CommonSenseQACustomOptionKeyCircular(CommonSenseQACustom, LikelihoodOptionKeyMultipleCircularChoiceTask):
    VERSION = 0
    
    def fewshot_context(self, doc, num_fewshot, **kwargs):
        description = f'The following are multiple choice questions (with answers).'
        kwargs["description"] = description
        return super().fewshot_context(doc=doc, num_fewshot=num_fewshot, **kwargs)

    def format_example(self, doc, keys, circular_index=0):
        """
        circular 0:
        <prompt>
        A. <choice1>
        B. <choice2>
        C. <choice3>
        D. <choice4>
        E. <choice5>
        Answer:

        circular 1:
        <prompt>
        A. <choice5>
        B. <choice4>
        C. <choice3>
        D. <choice2>
        E. <choice1>
        Answer:
        
        ...
        """

        choice_texts = doc["choices"]["text"]
        if circular_index == 0:
            choices = "".join(
                [f"{key}. {choice}\n" for key, choice in zip(keys, choice_texts)]
            )
        else:
            choices = ""
            for key_index, key in enumerate(keys):
                choices += f'{key}. {choice_texts[(key_index-circular_index)%len(keys)]}\n'

        # prompt = "Question: {question}\n A. {A}\n B. {B}\nC. {C}\nD. {D}\nE. {E}\nAnswer:"
        prompt = f"Question: {doc['question']}\n{choices}Answer:"
        return prompt
    
    def _process_doc(self, doc):
        keys = ["A", "B", "C", "D", "E"]
        out_doc = {
            "id": doc["id"],
            "choices": keys,
            "gold": keys.index(doc["answerKey"].strip()),
            "doc": doc,
        }
        return out_doc