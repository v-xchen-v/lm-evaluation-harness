from leaderboard.leaderboardtasks import LeaderBoardTask

results_save_root = r"C:\Users\xichen6\Documents\CodeSpace\eval_board\eval_results"
cache_folder = r"C:\Users\xichen6\Documents\CodeSpace\eval_board\autoeval_server\.cache"

# set the evaluation tasks should show on leaderboard table
leaderboard_tasks = [
    LeaderBoardTask(
        name="mmlu",
        abbr="MMLU(5 shot)",
        task_version=1,
        num_fewshot=5,
        use_cot=False,
        subtasks=[
            "hendrycksTest-abstract_algebra",
            "hendrycksTest-anatomy",
            "hendrycksTest-astronomy",
            "hendrycksTest-business_ethics",
            "hendrycksTest-clinical_knowledge",
            "hendrycksTest-college_biology",
            "hendrycksTest-college_chemistry",
            "hendrycksTest-college_computer_science",
            "hendrycksTest-college_mathematics",
            "hendrycksTest-college_medicine",
            "hendrycksTest-college_physics",
            "hendrycksTest-computer_security",
            "hendrycksTest-conceptual_physics",
            "hendrycksTest-econometrics",
            "hendrycksTest-electrical_engineering",
            "hendrycksTest-elementary_mathematics",
            "hendrycksTest-formal_logic",
            "hendrycksTest-global_facts",
            "hendrycksTest-high_school_biology",
            "hendrycksTest-high_school_chemistry",
            "hendrycksTest-high_school_computer_science",
            "hendrycksTest-high_school_european_history",
            "hendrycksTest-high_school_geography",
            "hendrycksTest-high_school_government_and_politics",
            "hendrycksTest-high_school_macroeconomics",
            "hendrycksTest-high_school_mathematics",
            "hendrycksTest-high_school_microeconomics",
            "hendrycksTest-high_school_physics",
            "hendrycksTest-high_school_psychology",
            "hendrycksTest-high_school_statistics",
            "hendrycksTest-high_school_us_history",
            "hendrycksTest-high_school_world_history",
            "hendrycksTest-human_aging",
            "hendrycksTest-human_sexuality",
            "hendrycksTest-international_law",
            "hendrycksTest-jurisprudence",
            "hendrycksTest-logical_fallacies",
            "hendrycksTest-machine_learning",
            "hendrycksTest-management",
            "hendrycksTest-marketing",
            "hendrycksTest-medical_genetics",
            "hendrycksTest-miscellaneous",
            "hendrycksTest-moral_disputes",
            "hendrycksTest-moral_scenarios",
            "hendrycksTest-nutrition",
            "hendrycksTest-philosophy",
            "hendrycksTest-prehistory",
            "hendrycksTest-professional_accounting",
            "hendrycksTest-professional_law",
            "hendrycksTest-professional_medicine",
            "hendrycksTest-professional_psychology",
            "hendrycksTest-public_relations",
            "hendrycksTest-security_studies",
            "hendrycksTest-sociology",
            "hendrycksTest-us_foreign_policy",
            "hendrycksTest-virology",
            "hendrycksTest-world_religions"
        ],
        metric="acc",
        aggregate_op='mean'
    ),
    LeaderBoardTask(
        name="mmlu",
        abbr="MMLU(0 shot)",
        task_version=1,
        num_fewshot=0,
        use_cot=False,
        subtasks=[
            "hendrycksTest-abstract_algebra",
            "hendrycksTest-anatomy",
            "hendrycksTest-astronomy",
            "hendrycksTest-business_ethics",
            "hendrycksTest-clinical_knowledge",
            "hendrycksTest-college_biology",
            "hendrycksTest-college_chemistry",
            "hendrycksTest-college_computer_science",
            "hendrycksTest-college_mathematics",
            "hendrycksTest-college_medicine",
            "hendrycksTest-college_physics",
            "hendrycksTest-computer_security",
            "hendrycksTest-conceptual_physics",
            "hendrycksTest-econometrics",
            "hendrycksTest-electrical_engineering",
            "hendrycksTest-elementary_mathematics",
            "hendrycksTest-formal_logic",
            "hendrycksTest-global_facts",
            "hendrycksTest-high_school_biology",
            "hendrycksTest-high_school_chemistry",
            "hendrycksTest-high_school_computer_science",
            "hendrycksTest-high_school_european_history",
            "hendrycksTest-high_school_geography",
            "hendrycksTest-high_school_government_and_politics",
            "hendrycksTest-high_school_macroeconomics",
            "hendrycksTest-high_school_mathematics",
            "hendrycksTest-high_school_microeconomics",
            "hendrycksTest-high_school_physics",
            "hendrycksTest-high_school_psychology",
            "hendrycksTest-high_school_statistics",
            "hendrycksTest-high_school_us_history",
            "hendrycksTest-high_school_world_history",
            "hendrycksTest-human_aging",
            "hendrycksTest-human_sexuality",
            "hendrycksTest-international_law",
            "hendrycksTest-jurisprudence",
            "hendrycksTest-logical_fallacies",
            "hendrycksTest-machine_learning",
            "hendrycksTest-management",
            "hendrycksTest-marketing",
            "hendrycksTest-medical_genetics",
            "hendrycksTest-miscellaneous",
            "hendrycksTest-moral_disputes",
            "hendrycksTest-moral_scenarios",
            "hendrycksTest-nutrition",
            "hendrycksTest-philosophy",
            "hendrycksTest-prehistory",
            "hendrycksTest-professional_accounting",
            "hendrycksTest-professional_law",
            "hendrycksTest-professional_medicine",
            "hendrycksTest-professional_psychology",
            "hendrycksTest-public_relations",
            "hendrycksTest-security_studies",
            "hendrycksTest-sociology",
            "hendrycksTest-us_foreign_policy",
            "hendrycksTest-virology",
            "hendrycksTest-world_religions"
        ],
        metric="acc",
        aggregate_op='mean'
    ),
    LeaderBoardTask(
        name="truthfulqa",
        abbr="TruthfulQA(0 shot)",
        task_version=1,
        num_fewshot=0,
        use_cot = False,
        subtasks=["truthfulqa_mc"],
        metric = "mc2",
        aggregate_op='mean'
    ),
    LeaderBoardTask(
        name="hellaswag",
        abbr="HellaSwag(10 shot)",
        task_version=0,
        num_fewshot=10,
        use_cot=False,
        subtasks=["hellaswag"],
        metric="acc_norm",
        aggregate_op='mean'
    ),
    LeaderBoardTask(
        name="arc",
        abbr="ARC(25 shot)",
        task_version=0,
        num_fewshot=25,
        use_cot=False,
        subtasks=["arc_challenge"],
        metric="acc_norm",
        aggregate_op='mean'
    ),
    LeaderBoardTask
    (
        name="agieval",
        abbr="AGIEval Eng QA(3-5 shot)",
        task_version=0,
        num_fewshot=5,
        use_cot=False,
        subtasks = [
            "agieval_eng_qa_lsat-ar",
            "agieval_eng_qa_lsat-lr",
            "agieval_eng_qa_lsat-rc",
            "agieval_eng_qa_logiqa-en",
            "agieval_eng_qa_sat-math",
            "agieval_eng_qa_sat-en",
            "agieval_eng_qa_aqua-rat",
            "agieval_eng_qa_sat-en-without-passage",
            "agieval_eng_qa_gaokao-english"
        ],
        metric="acc",
        aggregate_op='mean'
    ),
    LeaderBoardTask
    (
        name="agieval",
        abbr="AGIEval Eng QA(0 shot)",
        task_version=0,
        num_fewshot=0,
        use_cot=False,
        subtasks = [
            "agieval_eng_qa_lsat-ar",
            "agieval_eng_qa_lsat-lr",
            "agieval_eng_qa_lsat-rc",
            "agieval_eng_qa_logiqa-en",
            "agieval_eng_qa_sat-math",
            "agieval_eng_qa_sat-en",
            "agieval_eng_qa_aqua-rat",
            "agieval_eng_qa_sat-en-without-passage",
            "agieval_eng_qa_gaokao-english"
        ],
        metric="acc",
        aggregate_op='mean'
    )
]

LEADERBOARDTASK_REGISTRY = { x.abbr: x for x in leaderboard_tasks }

LEADERBOARDTASKS = list(LEADERBOARDTASK_REGISTRY.keys())

if __name__ == "__main__":
    print(LEADERBOARDTASKS)